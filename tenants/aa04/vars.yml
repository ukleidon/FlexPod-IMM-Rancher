---
######################################################################################################################
# 
# Key Variables used to buils many other variables instead of defining them manually.
#
######################################################################################################################
#
# !!!! Intersight API key and private key must be created before running the playbool.  !!!!
# !!!! This is only required if the Organization is diffent to the one used in ucs.yml  !!!!
# #1 Please logon to Intersight and create the Organization for this tenant if required.
# #2 Create a Role with administrator privileges in the Organization.
# #3 Assign the new role to your User and switch to the new role
# #4 Create a new API key with the created role and enter the information here.
#
api_key_id        : 5c8bbb507564612d307b0837/5c8bb9d67564612d307ae010/663e042c756461310146634b
api_private_key   : Keys/AA04.txt

#
# TENANT NAME is used i.e. as name for the VRF, as prefix in Intersight and so on.
# It is recommended to use only letters and numbers and no special characters.
tenant_name:   'AA04'

# Tenand ID (tid) is used as digit in various pools like UUID, MAC, WWN,...
# The tid must be a number between 00 and 99
# like for mac_pool_a - 00:25:B5:A0:{{ tid }}:[pool]
tid:           '01'

# How many server profiles to create?
# num_profiles defines the number of server profiles created based on the server_profile_template
# defined for this tenant.
num_profiles:  0

# Are there virtual tenants on top of this physical tenant?
# This parameter is used to collect the required information for vtenants for deployment
# vtenants:  true or false
vtenants:      true

# With UCS the default network option used vNIC failover on the VIC card. There is no need to configure
# a bond device on the OS level.
# If you like or if the solution requires the use of bonding on the OS layer, set os_bond to true.
# This will disable vNIC failover and creates two vNICs for the same VLANs, one on fabric A and one
# on fabric B. The native VLAN ID on the vNICs is set to native_vlan_id. It is required to createa
# VLAN interfaces on the OS to access the network.
# Default is false
os_bond           : 'true'

# To configure local disk boot set configure_local to true otherwise to false
# This variable has no impact to configure_iscsi or configure_fc.
# Default is false
configure_local   : 'false'

# The OS Type variable is used to defime BIOS policies, adapter policies, and LUNs.
# IT can be linux (for all kinds of linux and K8s), windows, and vmware.
os_type:       'linux'  

# lan_state should be set to 'present' to configure the objects
# In future, this parameter will be used for deleting the configuration
#
lan_state : 'present'

# Proxy settings
#
#proxys:
#  - 'http_proxy: "http://172.16.4.12:81"'
#  - 'https_proxy: "http://172.16.4.12:81"'

#
# Local User and password are used in Intersight and for the SVM admin
t_name_of_local_user                  : "fpadmin"
t_password_for_local_user             : "H1ghV0lt"

# Vmedia file location used for UCS to define vmedia policy for first boot
# CCD is for the boot ISO image.
# HDD is for an disk image to store autoinstall file or additional packages
vmedia_file_location_cdd          : "http://10.104.1.4:8080/harvester-v1.3.2-amd64.iso"
vmedia_name_cdd                   : 'Harvester-1.3.2'
  #vmedia_file_location_hdd          : "http://10.104.1.4:8080/autoinstall.img"
#vmedia_name_hdd                   : 'AUTOINSTALL'
#
#---------------------------------------------------------------------------------------------------------------------
#--------------------- NetApp Storage variables ------------------------------
#
# The following variables must change after ontap_config_part_1 run
# and ucs_config run.
#
# Storage iSCSI Target IQN from the NetApp SVM created for this tenant.
#
storage_iscsi_IQN: 'iqn.1992-08.com.netapp:sn.3e884cbadd3d11ec819ad039ea29d44a:vs.6'
#
# Storage FCP WWPNs (capture from storage system)
#
fcp_lif_01a: '20:26:d0:39:ea:29:ce:d4'
fcp_lif_02a: '20:23:d0:39:ea:29:ce:d4'
fcp_lif_01b: '20:25:d0:39:ea:29:ce:d4'
fcp_lif_02b: '20:24:d0:39:ea:29:ce:d4'
#
# Storage FC-NVMe WWPNs (capture from storage system)
#
fc_nvme_lif_01a: '20:06:00:a0:98:e2:17:ca'
fc_nvme_lif_02a: '20:08:00:a0:98:e2:17:ca'
fc_nvme_lif_01b: '20:07:00:a0:98:e2:17:ca'
fc_nvme_lif_02b: '20:09:00:a0:98:e2:17:ca'
#
#----------------------------------------------------------------------------------------------------------------------------
#------------------------- Rancher RKE2 variables ----------------------------------
#
install_rke2_version: v1.27.12+rke2r1
#
#############################################################################################################################
# 
# Network information for this Tenant - Mandatory information used to configure all components ...
#
#############################################################################################################################
# This is the list of all the VLANs that will be defined on Nexus, Storage, UCS, and Hypervisor
# VLAN Names are adjustable and can be modified in here
# Comment out any VLANs that are not used here and below (iSCSI and NVMe-TCP)
# t_XXXX_name          : Defines the Name of the network
# t_XXXX_id            : Defines the VLAN ID of the network
# t_XXXX_network_prefix: Defines the first three digits of the network ip address. 
#                        Host IPs, Storage IPs, and Routing IPs are automaticaliy added to the prefix
#                        Router IP is set as default to .1
#                        SVI IP on n9kA is .2 and on n9kB is .2
#                        NetApp SVM IPs are set to .101 o node 1 and .102 on node 2.
# t_XXXX_network_mask  : Defines the netmask of the network
#
t_wan_vlan_name:           "{{ tenant_name }}-Management_LAN" # IN-BAND Management VLAN
t_wan_vlan_id:             2004                               # VLAN ID for IN-BAND Management
t_wan_network_prefix:      '192.168.177'                      # !!! EDIT !!! first three digits of the network adress
t_wan_network_mask:        24                                 # Network CIDR for this network
t_wan_network_netmask:     255.255.255.0                      # Netmask for this network
#
t_ib_vlan_name:           "{{ tenant_name }}-Management_LAN" # IN-BAND Management VLAN
t_ib_vlan_id:             1041                               # VLAN ID for IN-BAND Management
t_ib_network_prefix:      '10.104.1'                         # !!! EDIT !!! first three digits of the network adress
t_ib_network_mask:        24                                 # Network CIDR for this network
t_ib_network_netmask:     255.255.255.0                      # Netmask for this network
#
t_iscsiA_vlan_name:       "{{ tenant_name }}-iSCSI-A"        # iSCSI-A VLAN (if needed)
t_iscsiA_vlan_id:         1048                               # Management VLAN +1
t_iscsiA_network_prefix:  '10.104.8'                         # !!! EDIT !!! first three digits of the network adress
t_iscsiA_network_mask:    24                                 # Network CIDR for this network
t_iscsiA_network_netmask: 255.255.255.0                      # Netmask for this network
#
t_iscsiB_vlan_name:       "{{ tenant_name }}-iSCSI-B"        # iSCSI-B VLAN (if needed)
t_iscsiB_vlan_id:         1049                               # Management VLAN +2
t_iscsiB_network_prefix:  '10.104.9'                         # !!! EDIT !!! first three digits of the network adress
t_iscsiB_network_mask:    24                                 # Network CIDR for this network
t_iscsiB_network_netmask: 255.255.255.0                      # Netmask for this network
#
t_nfs_vlan_name:          "{{ tenant_name }}-NFS"            # NFS Traffic between ESXi and Storage
t_nfs_vlan_id:            1043                               # Management VLAN +3
t_nfs_network_prefix:     '10.104.3'                         # !!! EDIT !!! first three digits of the network adress
t_nfs_network_mask:       24                                 # Network CIDR for this network
t_nfs_network_netmask:    255.255.255.0                      # Netmask for this network
#
t_access_vlan_name:       "{{ tenant_name }}-Access-Traffic" # VLAN to carry Access traffic to RKE2 payload
t_access_vlan_id:         1042                               # Management VLAN +4
t_access_network_prefix:  '10.104.2'                         # !!! EDIT !!! first three digits of the network adress
t_access_network_mask:    24                                 # Network CIDR for this network
t_access_network_netmask: 255.255.255.0                      # Netmask for this network
#
t_nvme_tcpA_vlan_name:       "{{ tenant_name }}-Infra-NVMe-TCP-A" # NVMe-TCP-A VLAN (if needed)
t_nvme_tcpA_vlan_id:         2098                                 # Management VLAN +7
t_nvme_tcpA_network_prefix:  '172.18.5'                           # first three digits of the network adress
t_nvme_tcpA_network_mask:    24                                   # Netmask
t_nvme_tcpA_network_netmask: 255.255.255.0                        # Netmask for this network
#
t_nvme_tcpB_vlan_name:       "{{ tenant_name }}-Infra-NVMe-TCP-B" # NVMe-TCP-B VLAN (if needed)
t_nvme_tcpB_vlan_id:         2099                                 # Management VLAN +8
t_nvme_tcpB_network_prefix:  '172.18.6'                           # first three digits of the network adress
t_nvme_tcpB_network_mask:    24                                   # Network CIDR for this network
t_nvme_tcpB_network_netmask: 255.255.255.0                        # Netmask for this network


#
#----------------------------------------------------------------------------------------------------------------------------
#
# VLAN Lists - Comment out or remove any VLANs not being used.
#
# The ib_mgmt_vlan_list contains one entry, the IB-MGMT VLAN.
#
t_ib_mgmt_vlan_list:
  - name: "{{ t_ib_vlan_name }}"
    id: "{{ t_ib_vlan_id }}"
    native: 'no'
    state: "{{lan_state}}"
#
# The storage_vlan_list contains VLANs that are configured on the storage controllers.
# These VLANs are also configured in the UCS and in the Nexus switches. This list has
# two extra fields, storage_protocol, and fabric. Do not remove these extra fields.
#
# !!! EDIT !!!!
t_storage_vlan_list:
  - name: "{{ t_nfs_vlan_name }}"
    id: "{{ t_nfs_vlan_id }}"
    native: 'no'
    storage_protocol: NFS
    state: "{{lan_state}}"
    # ISCSI A and B VLANs should be deleted or commended out for Fiber-Channel-Only deployments
  - name: "{{ t_iscsiA_vlan_name }}"
    id: "{{ t_iscsiA_vlan_id }}"
    native: 'no'
    storage_protocol: iSCSI
    fabric: A
    state: "{{lan_state}}"
  - name: "{{ t_iscsiB_vlan_name }}"
    id: "{{ t_iscsiB_vlan_id }}"
    native: 'no'
    storage_protocol: iSCSI
    fabric: B
    state: "{{lan_state}}"

# The remaining_vlan_list contains all vlans that are not configured on storage,
# but are configured in Nexus and UCS.
#
# !!! EDIT !!!!
t_remaining_vlan_list:
  - name: "{{ t_access_vlan_name }}"
    id: "{{ t_access_vlan_id }}"
    native: 'no'
    state: "{{lan_state}}"

#
# VLANs definitions below will be used to setup trunk ports on Nexus Switches
# all_vlans_list: for vpc_peer_link and UCS FI trunk ports
# These VLANs must be same or a subset of the vlan_list abovea
# Comment out or remove any VLANs that are not used (iSCSI and NVMe-TCP)
# In future, these values will be auto-generated
#
# !!! EDIT !!!!
# storage_vlans_list: for storage uplink trunk ports
t_storage_vlans_list: "{{ t_ib_vlan_id }},{{ t_nfs_vlan_id }},{{ t_iscsiA_vlan_id }},{{ t_iscsiB_vlan_id }}"
# mgmt_vlans_list: for uplink/management switch trunk port
t_mgmt_vlans_list: "{{ t_ib_vlan_id }},{{ t_access_vlan_id }}"
# access: for uplink/management switch trunk port
t_access_vlans_list: "{{ t_access_vlan_id }}"
#
t_all_vlans_list: "{{ all_vlans_list }},{{ t_mgmt_vlans_list }},{{ t_access_vlans_list }},{{ t_storage_vlans_list }}"
#
#
#----------------------------------------------------------------------------------------------------------------------------
#
# The SVI list defines IP and VRF information to enable routing function on the Nexus Switches.
# The baseIP is defined in the nexus host configuration file in the hosts_var directory.
#
t_svi_list:
  # name   : VLAN ID variable
  # vrf    : Default is {{ tenant_name }}, can be changed per network
  # address: Default is {{ network_prefix}} + {{ baseIP }} / {{ network_mask }}
  #                        172.18.0         .     2        /     24
  # hsrp   : HSRP cluster IP. Default is {{ network_prefix }} + 1
  #                                           172.18.0        . 1
# !!! EDIT !!!!
#  - name:    "{{ t_ib_vlan_id }}"
#    vrf:     "{{ tenant_name }}"
#    address: "{{ t_ib_network_prefix}}.{{ baseIP }}/{{ t_ib_network_mask }}"
#    hsrp:    "{{ t_ib_network_prefix}}.1"
  - name:    "{{ t_iscsiA_vlan_id }}"
    vrf:     "{{ tenant_name }}"
    address: "{{ t_iscsiA_network_prefix }}.{{ baseIP }}/{{ t_iscsiA_network_mask }}"
    hsrp:    "{{ t_iscsiA_network_prefix }}.250"
  - name:    "{{ t_iscsiB_vlan_id }}"
    vrf:     "{{ tenant_name }}"
    address: "{{ t_iscsiB_network_prefix }}.{{ baseIP }}/{{ t_iscsiB_network_mask }}"
    hsrp:    "{{ t_iscsiB_network_prefix }}.250"
  - name:    "{{ t_nfs_vlan_id }}"
    vrf:     "{{ tenant_name }}"
    address: "{{ t_nfs_network_prefix }}.{{ baseIP }}/{{ t_nfs_network_mask }}"
    hsrp:    "{{ t_nfs_network_prefix }}.250"
  - name:    "{{ t_access_vlan_id }}"
    vrf:     "{{ tenant_name }}"
    address: "{{ t_access_network_prefix }}.{{ baseIP }}/{{ t_access_network_mask }}"
    hsrp:    "{{ t_access_network_prefix }}.250"



#----------------------------------------------------------------------------------------------------------------------------
#
# VSAN Parameters - Configure only if VSAN IDs are different as configured in group_vars/ucs.yml
#
vsan_A_name: '{{ tenant_name }}-FlexPod-Fabric-A'
vsan_A_id: 101
vsan_A_fcoe_vlan: 101
#
vsan_B_name: '{{ tenant_name }}-FlexPod-Fabric-B'
vsan_B_id: 102
vsan_B_fcoe_vlan: 102
#
#
#############################################################################################################################
#
# NetApp Storage informantion for this Tenant
#
#############################################################################################################################
#
#----------------------------------------------------------------------------------------------------------------------------
#
# Host Names - Separated by either FC or iSCSI. If not using the protocol, comment out the list entries.
# These host names and identifiers will be used for NetApp LUN and igroup names and Cisco MDS Device Alias names 
# and zone entries.
# The hostnames can also be assigned as UCS Profile names.

t_iscsi_hosts:
  - hostname: "{{ tenant_name }}-01"
    iscsi_iqn: "iqn.2010-11.com.flexpod:{{ tenant_name }}-ucshost:1"
    nvme_nqn: "nqn.2014-08.com.cisco.flexpodb4:nvme:{{ tenant_name }}-01"
#
# Storage NVMe-TCP Target Interfaces
#
nvme_lif_01a: "{{ t_nvme_tcpA_network_prefix }}.1"
nvme_lif_01b: "{{ t_nvme_tcpB_network_prefix }}.1"
nvme_lif_02a: "{{ t_nvme_tcpA_network_prefix }}.2"
nvme_lif_02b: "{{ t_nvme_tcpB_network_prefix }}.2"
#
# Storage iSCSI Target Interfaces
#
iscsi_lif_01a: "{{ t_iscsiA_network_prefix }}.1"
iscsi_lif_01b: "{{ t_iscsiB_network_prefix }}.1"
iscsi_lif_02a: "{{ t_iscsiA_network_prefix }}.2"
iscsi_lif_02b: "{{ t_iscsiB_network_prefix }}.2"
#
# Storage NFS Target Interfaces
#
nfs_lif_01: "{{ t_nfs_network_prefix }}.1"
nfs_lif_02: "{{ t_nfs_network_prefix }}.2"
#
#----------------------------------------------------------------------------------------------------------------------------
#
# NetApp Storage Virtual Machine informantion
#
cluster_name     : 'AA07-A400'                   # NetApp cluster name, also used as ansible_host in inventory !!!
aggr_prefix      : 'AA07-A400'                   # Should be same as cluster name
svm_specs:
  svm_name         : "AA04-rke2-SVM"
    #  svm_name         : "{{ tenant_name }}_svm"
  svm_root_vol     : "{{ tenant_name }}_svm_root"
  allowed_protocols:
    #provide the values in lower case only, supported options for this solution are nfs, fcp, iscsi, nvme
    #For FC-NVMe config, use fcp and nvme
    #For NVMe/TCP config, use nvme and iscsi
    - nfs
#    - fcp  
#    - nvme
    - iscsi
  data_protocol: nfs
  client_match: "{{ t_nfs_network_prefix }}.0/{{ t_nfs_network_mask }}"
  data_volumes_file: # If name includes swap - volume efficientcy will be disabled
    - {name: "{{tenant_name}}_datastore", size: 1024, residing_aggr: "{{ aggr_prefix }}n2_aggr1"}
      #    - {name: "{{tenant_name}}_swap", size: 1024, residing_aggr: "{{ aggr_prefix }}n2_aggr1"}
      #    - {name: "{{tenant_name}}_vCLS", size: 100, residing_aggr: "{{ aggr_prefix }}n2_aggr1"}
  data_volumes_block:
    - {name: "{{tenant_name}}_boot", size: 1024, residing_aggr: "{{ aggr_prefix }}n2_aggr1"}
    - {name: infra_data, size: 10, residing_aggr: "{{ aggr_prefix }}n2_aggr1"}
      #    - {name: "{{tenant_name}}_nvme", size: 1024, residing_aggr: "{{ aggr_prefix }}n2_aggr1"}
  nvme_subsystem: nvme_infra_hosts
    #  nvme_namespaces:  #Mention the namespaces that you want to create and map to the subsystem
    #    - {name: "{{tenant_name}}_nvme_01", size: 500, residing_vol: "{{tenant_name}}_nvme"}
  #host names will be used for NetApp Boot LUNs and igroup names. These vars are placed under group_vars/all.yml file
  boot_luns_iscsi: {size: 400, residing_vol: "{{tenant_name}}_boot"}
  data_luns_iscsi: {name: dataspace_data, size: 4, residing_vol: infra_data, host: 'test01'}
  svm_mgmt_lif: {home_node: "{{ cluster_name }}n2", address: "{{ t_ib_network_prefix }}.101", netmask: 255.255.255.0, 
                  gateway: "{{ t_ib_network_prefix }}.254", lif_name: "{{ tenant_name }}-svm-mgmt"}
  vsadmin_password: "{{ t_password_for_local_user }}"
  os_type: linux
  dns_server_svm:
    - "{{ dns_servers[0].ip_address }}"
      #    - "{{ dns_servers[1].ip_address }}"
  dns_domain_svm: "{{ dns_domain_name }}"
  svm_login_banner: This SVM is reserved for authorized users only!  #SVM Login banner Text message
  audit_log_volume_specs: {name: "{{tenant_name}}_audit_log", size: 50, residing_aggr: "{{ aggr_prefix }}n1_aggr1"} 
       #Audit log destination volume where consolidated audit logs will be stored

svm_node_specs:
  - node_name: "{{ cluster_name }}n1"
    nfs_lifs:  {name: "{{ tenant_name }}-nfs-lif-01a", address: "{{ nfs_lif_01 }}", netmask: 255.255.255.0}
    fcp_lifs:    #Fill out this value only if fcp will be mentioned under allowed_protocols in svm_specs
      - {name: "{{ tenant_name }}-fcp-lif-01a", home_port: 5a, fabric: A}  #Do not change the fabric ID
      - {name: "{{ tenant_name }}-fcp-lif-01b", home_port: 5b, fabric: B}  #Do not change the fabric ID
    fc-nvme_lifs:    #Fill out this value only if fcp and nvme will be mentioned under allowed_protocols in svm_specs
      - {name: "{{ tenant_name }}-fc-nvme-lif-01a", home_port: 2a, fabric: A}  #Do not change the fabric ID
      - {name: "{{ tenant_name }}-fc-nvme-lif-01b", home_port: 2b, fabric: B}  #Do not change the fabric ID
    iscsi_lifs:  #Fill out this value only if iscsi will be mentioned under allowed_protocols in svm_specs. 
                                                                                        #Do not change the fabric ID
      - {name: "{{ tenant_name }}-iscsi-lif-01a", address: "{{ iscsi_lif_01a }}", netmask: 255.255.255.0, fabric: A}  
      - {name: "{{ tenant_name }}-iscsi-lif-01b", address: "{{ iscsi_lif_01b }}", netmask: 255.255.255.0, fabric: B}
    nvme_tcp_lifs:  #Fill out this value only if iscsi and nvme will be mentioned under allowed_protocols 
                                                                                          #Do not change the fabric ID
      - {name: "{{ tenant_name }}-nvme-tcp-lif-01a", address: "{{ nvme_lif_01a }}", netmask: 255.255.255.0, fabric: A}
      - {name: "{{ tenant_name }}-nvme-tcp-lif-01b", address: "{{ nvme_lif_01b }}", netmask: 255.255.255.0, fabric: B}
  - node_name: "{{ cluster_name }}n2"
    nfs_lifs:  {name: "{{ tenant_name }}-nfs-lif-02b", address: "{{ nfs_lif_02 }}", netmask: 255.255.255.0}
    fcp_lifs:    #Fill out this value only if fcp will be mentioned under allowed_protocols in svm_specs
      - {name: "{{ tenant_name }}-fcp-lif-02a", home_port: 5a, fabric: A}  #Do not change the fabric ID
      - {name: "{{ tenant_name }}-fcp-lif-02b", home_port: 5b, fabric: B}  #Do not change the fabric ID
    fc-nvme_lifs:    #Fill out this value only if fcp and nvme will be mentioned under allowed_protocols 
      - {name: "{{ tenant_name }}-fc-nvme-lif-02a", home_port: 2a, fabric: A}  #Do not change the fabric ID
      - {name: "{{ tenant_name }}-fc-nvme-lif-02b", home_port: 2b, fabric: B}  #Do not change the fabric ID
    iscsi_lifs:  #Fill out this value only if iscsi will be mentioned under allowed_protocols in svm_specs. 
                                                                                          #Do not change the fabric ID
      - {name: "{{ tenant_name }}-iscsi-lif-02a", address: "{{ iscsi_lif_02a }}", netmask: 255.255.255.0, fabric: A}
      - {name: "{{ tenant_name }}-iscsi-lif-02b", address: "{{ iscsi_lif_02b }}", netmask: 255.255.255.0, fabric: B}
    nvme_tcp_lifs:  #Fill out this value only if iscsi and nvme will be mentioned under allowed_protocols 
                                                                                          #Do not change the fabric ID
      - {name: "{{ tenant_name }}-nvme-tcp-lif-02a", address: "{{ nvme_lif_02a }}", netmask: 255.255.255.0, fabric: A}
      - {name: "{{ tenant_name }}-nvme-tcp-lif-02b", address: "{{ nvme_lif_02b }}", netmask: 255.255.255.0, fabric: B}


#
#############################################################################################################################
# 
# Cisco UCS / Intersight Section 
#
#############################################################################################################################

# This file contains Tenant specific UCS IMM variables in addition to ucs.yml .
# 
# Prefix added to the pool/policy/profile configuration to eaisly identify items created by Ansible
prefix        : "{{ tenant_name }}"

# Organization Name in Intersight !! The Organization MUST exist !!
org_name      : "{{ tenant_name }}"

# Chassis Configuration - UUID Pool
name_of_uuid_pool: '{{ tenant_name }}--UUID-Pool'
uuid_prefix: 'AA{{ tid }}0000-0000-0001'
uuid_size: 99
uuid_from: 'AA00-000000000001'

#----------------------------------------------------------------------------------------------------------------------------
#------------------------- Various Ethernet and IP  Pools ------------------------------
#
#
# IP Address Pool for Management Access (out-of-band access)
# Remove the lines if no OOB Management IP pool is needed.
# 
#oob_name_of_ip_pool_for_management_access       : "{{ tenant }}-OOB-MGMT-IP-Pool"
oob_name_of_ip_pool_for_management_access       : "HARV-OOB-MGMT-IP-Pool"
oob_management_ip_pool_details                  : "{{ tenant }}-OOB-MGMT-IP-Pool"
oob_ip_pool_start_for_management_access         : 10.104.0.200
oob_size_of_ip_pool_for_management_access       : 30
oob_gateway_mgmt                                : 10.104.0.254
oob_netmask_mgmt                                : 255.255.255.0
oob_primary_dns_mgmt                            : "{{ dns_servers[0].ip_address }}"
oob_secondary_dns_mgmt                          : "{{ dns_servers[1].ip_address }}"

#
# Make sure that the IP addresses are not overlapping with the IB-MBMT-Pool
# defined in ucs.yml.
# The definition of an Tenant internal IB-MGMT-IP-Pool is optional.
# Default is to use the global IB-MGMT-IP-Pool
#
#name_of_ip_pool_for_management_access       : "{{ prefix }}-IB-MGMT-IP-Pool"
name_of_ip_pool_for_management_access       : "HARV-IB-MGMT-IP-Pool"
ip_pool_start_for_management_access         : "{{ t_ib_network_prefix }}.211"
size_of_ip_pool_for_management_access       : 15
gateway_mgmt                                : "{{ t_ib_network_prefix }}.1"
netmask_mgmt                                : "{{ t_ib_network_netmask }}"
primary_dns_mgmt                            : "{{ dns_servers[0].ip_address }}"
secondary_dns_mgmt                          : "{{ dns_servers[1].ip_address }}"

#
# iSCSI Pools : only configured for iSCSI i.e. when configure_iscsi == 'true'
# iSCSI IQN are defined in ucs.yml
#
# IP Pool - iSCSI-A; Use 0.0.0.0 as gateway address if no gateway is present
name_of_ip_pool_for_iscsi_a           : "{{ prefix }}-iSCSI-A-IP-Pool"
ip_pool_start_iscsi_a                 : "{{ t_iscsiA_network_prefix }}.10"
size_of_ip_pool_iscsi_a               : 128
gateway_iscsi_a                       : "0.0.0.0"
netmask_iscsi_a                       : "{{ t_iscsiA_network_netmask }}"

# IP Pool - iSCSI-B; Use 0.0.0.0 as gateway address if no gateway is present
name_of_ip_pool_for_iscsi_b           : "{{ prefix }}-iSCSI-B-IP-Pool"
ip_pool_start_iscsi_b                 : "{{ t_iscsiB_network_prefix }}.10"
size_of_ip_pool_iscsi_b               : 128
gateway_iscsi_b                       : "0.0.0.0"
netmask_iscsi_b                       : "{{ t_iscsiB_network_netmask }}"

# IQN Pool
name_of_iqn_pool                      : "{{ prefix }}-IQN-Pool"
prefix_for_iqn                        : "iqn.2010-11.com.flexpod"
suffix_for_iqn                        : "{{ prefix | lower }}-ucshost"
iqn_start                             : 01
size_of_iqn_pool                      : 128

#
# Separate MAC Address Pools for Fabric-A and Fabric-B
#For FI-A
name_of_mac_pool_on_fi_a          : "{{ prefix }}-Mac-Pool-A"
mac_pool_start_on_fi_a            : "00:25:B5:A0:{{tid}}:00"
size_of_mac_pool_on_fi_a          : 128

# For FI-B
name_of_mac_pool_on_fi_b          : "{{ prefix}}-Mac-Pool-B"
mac_pool_start_on_fi_b            : "00:25:B5:B0:{{tid}}:00"
size_of_mac_pool_on_fi_b          : 128

#----------------------------------------------------------------------------------------------------------------------------
#------------------------- Various SAN Pools ------------------------------
#
# Make sure that the WWxN addresses are not overlapping with the Pools
# defined in ucs.yml.
#
# Fibre Channel Pools : only needed/created when configure_fc == 'true'
#
# WWNN Pool
#
name_of_wwnn_pool               : "{{ prefix }}-WWNN-Pool"
wwnn_pool_start                 : "20:00:00:25:B5:{{tid}}:00:00"
wwnn_pool_size                  : 128
#
# WWPN Pool
#
#For FI-A
name_of_wwpn_pool_san_a         : "{{ prefix }}-WWPN-Pool-A"
wwpn_pool_start_on_san_a        : "20:00:00:25:B5:A0:{{tid}}:00"
wwpn_pool_size_on_san_a         : 128
#
#For FI-B
name_of_wwpn_pool_san_b         : "{{ prefix }}-WWPN-Pool-B"
wwpn_pool_start_on_san_b        : "20:00:00:25:B5:B0:{{tid}}:00"
wwpn_pool_size_on_san_b         : 128

#----------------------------------------------------------------------------------------------------------------------------
#------------------------- Server Policies ------------------------------
#
# BIOS policies
name_of_bios_policy           : "{{ name_of_m7_bios_policy }}"
description_of_bios_policy    : "{{ description_of_m7_bios_policy }}"
name_of_m6_bios_policy           : "{{ prefix }}-{{ server_cpu_type }}-M6-Linux-BIOS"
description_of_m6_bios_policy    : "BIOS Policy for Performance Optimized {{ server_cpu_type }}-based M6 Linux Servers"
name_of_m5_bios_policy        : "{{ prefix }}-Intel-M5-Linux-BIOS"
description_of_m5_bios_policy : "BIOS Policy for Performance Optimized Intel-based M5 ESXi Servers"
name_of_m7_bios_policy        : "{{ prefix }}-Intel-M7-Linux-BIOS"
description_of_m7_bios_policy : "BIOS Policy for Performance Optimized Intel-based M7 Linux Servers"
#
# Virtual Media Policy - enables mounting ISO files using KVM 
name_of_vmedia_policy                 : "{{ prefix }}-KVM-Mount-vMedia"
vmedia_protocol                       : "http"
vmedia_device_type_cdd                : "cdd"
vmedia_device_type_hdd                : "hdd"
description_of_vmedia_policy          : "vMedia Policy to enable KVM mounted DVDs"

# Local User Policy to enable KVM and IPMI Access
name_of_LocalUser_policy            : "{{ prefix }}-LocalUser-Policy"
description_of_LocalUser_policy     : "Allow Local Users in this policy KVM Access to Servers"

# Ethernet QoS Policy to set MTU 9000 and QoS Best Effort;
# QoS values can be updated in roles/create_server_policies/defaults/main.yml
name_of_ethernet_qos_policy         : "{{ prefix }}-EthernetQoS-Policy"

# Default Ethernet Adapter Policy for Linux
# Various values can be updated under roles/create_server_policies/defaults/main.yml
name_of_ethernet_adapter_linux_policy     : "{{ prefix }}-EthAdapter-Linux-Policy"
name_of_ethernet_adapter_linuxroce_policy : "{{ prefix }}-EthAdapter-Linux-RoCE-Policy"

# Default Ethernet Adapter Policy for Windows
# Various values can be updated under roles/create_server_policies/defaults/main.yml
name_of_ethernet_adapter_win_policy     : "{{ prefix }}-EthAdapter-Linux-Policy"
name_of_ethernet_adapter_winroce_policy : "{{ prefix }}-EthAdapter-Linux-RoCE-Policy"

# Default Ethernet Adapter Policy for VMware
# Various values can be updated under roles/create_server_policies/defaults/main.yml
name_of_ethernet_adapter_esx_policy          : "{{ prefix }}-EthAdapter-Linux-Policy"
name_of_ethernet_adapter_esx_hightrf_policy  : "{{ prefix }}-EthAdapter-Linux-HighTrf"
name_of_ethernet_adapter_16rxq_policy        : "{{ prefix }}-EthAdapter-16RXQs-{{ vic_type }}"

# Default Ethernet Adapter Policy for Profile definition
# Various values can be updated under roles/create_server_policies/defaults/main.yml
name_of_ethernet_adapter_policy          : "{{ name_of_ethernet_adapter_linux_policy }}"
name_of_ethernet_adapter_hightrf_policy  : "{{ name_of_ethernet_adapter_linuxroce_policy  }}"

# Ethernet Network Group Policy - VLAN details of vNICs assigned for management VLANs
name_of_network_group_policy_for_wan         : "{{ prefix }}-wan-NetGrp-Policy"
description_of_network_group_policy_wan  : "VLAN details of access interfaces to allow wan traffic"

# Ethernet Network Group Policy - VLAN details of vNICs assigned for management VLANs
name_of_network_group_policy_for_management         : "{{ prefix }}-management-NetGrp-Policy"
description_of_network_group_policy_for_management  : "VLAN details of access interfaces to allow management traffic"

# Ethernet Network Group Policy - VLAN details of vNICs assigned for access VLANs
name_of_network_group_policy_for_access         : "{{ prefix }}-access-NetGrp-Policy"
description_of_network_group_policy_for_access  : "VLAN details of access interfaces to allow access traffic"

# Ethernet Network Group Policy - VLANs details of vNIC assigned for NFS and iSCSI VLANs
name_of_network_group_policy_for_storage          : "{{ prefix }}-storage-NetGrp-Policy"
description_of_network_group_policy_storage       : "VLAN details of storage interfaces to allow NFS and iSCSI"

# Ethernet Network Group Policies for iSCSI interfaces (only required when configure_iscsi== true) 
# Ignore the configuration for iSCSI-A and iSCSI-B Network Group policies if iSCSI is not configured

# Ethernet Network Group Policy - VLAN details of iscsi-a vNIC Interface
# iSCSI-A VLAN will be set as Native VLAN - NVMe-TCP-A VLAN Added if using NVMe-TCP
name_of_network_group_policy_for_iscsi_a                : "{{ prefix }}-iSCSI-A-NetGrp-Policy"

# Ethernet Network Group Policy - VLAN details of iscsi-b vNIC Interface
# iSCSI-B allowed VLAN will be set as Native VLAN - NVMe-TCP-B VLAN Added
name_of_network_group_policy_for_iscsi_b                : "{{ prefix }}-iSCSI-B-NetGrp-Policy"

# LAN Connectivity Policy
# 4 interfaces for all hosts. 2 additional interfaces for iSCSI boot from SAN

# - Note -
# In the current version of Ansible Playbooks, we require unique vNIC names when 
# configuring both iSCSI and FC LAN connectivity policies

name_of_iscsi_lan_connectivity_policy         : "{{ prefix }}-iSCSI-Boot-{{ vic_type }}-LANCon"
description_of_iscsi_lan_connectivity_policy  : "LAN Connectivity Policy for iSCSI boot from SAN - {{ vic_type }} VIC"
name_of_fc_lan_connectivity_policy            : "{{ prefix }}-FC-Boot-{{ vic_type }}-LANCon"
description_of_fc_lan_connectivity_policy     : "LAN Connectivity Policy for FC boot from SAN - {{ vic_type }} VIC"
name_of_lan_connectivity_policy               : "{{ prefix }}-{{ vic_type }}-LANCon"
description_of_lan_connectivity_policy        : "LAN Connectivity Policy for local disk boot - {{ vic_type }} VIC"

# First vNIC is used for all MTU1500 traffic - or limited to management traffic
name_of_vnic_1                                : "{{ prefix | lower}}-mgmt-{{ vic_type }}"
# Second vNIC is used for all MTU900 traffic especially for NFS storage traffic or for storage replication in HCI
name_of_vnic_2                                : "{{ prefix | lower}}-storage-{{ vic_type }}"
# Third vNIC is optional used for user and application traffic
# Please add or remove leading # based on your needs
#name_of_vnic_3                                : "{{ prefix | lower}}-access-{{ vic_type }}"
# Forth vNIC is used for special use-cases
# Please add or remove leading # based on your needs
#name_of_vnic_4                                : "{{ prefix | lower}}-wan-{{ vic_type }}"

# First vNIC is used for all MTU1500 traffic - or limited to management traffic
name_of_iscsi_vnic_1                          : "{{ prefix | lower}}-iscsi-mgmt-{{ vic_type }}-A"
# Second vNIC is used for all MTU900 traffic especially for NFS storage traffic or for storage replication in HCI
name_of_iscsi_vnic_2                          : "{{ prefix | lower}}-iscsi-stoage-{{ vic_type }}-B"
# Third vNIC is optional used for user and application traffic
# Please add or remove leading # based on your needs
#name_of_iscsi_vnic_3                          : "{{ prefix | lower}}-iscsi-access-{{ vic_type }}-A"
# Forth vNIC is used for special use-cases
# Please add or remove leading # based on your needs
#name_of_iscsi_vnic_4                          : "{{ prefix | lower}}-iscsi-wan-{{ vic_type }}-B"

# First vNIC is used for all MTU1500 traffic - or limited to management traffic
name_of_fc_vnic_1                             : "{{ prefix | lower}}-fc-mgmt-{{ vic_type }}-A"
# Second vNIC is used for all MTU900 traffic especially for NFS storage traffic or for storage replication in HCI
name_of_fc_vnic_2                             : "{{ prefix | lower}}-fc-storage-{{ vic_type }}-B"
# Third vNIC is optional used for user and application traffic
# Please add or remove leading # based on your needs
#name_of_fc_vnic_3                             : "{{ prefix | lower}}-fc-access-{{ vic_type }}-B"
# Forth vNIC is used for special use-cases
# Please add or remove leading # based on your needs
#name_of_fc_vnic_4                          : "{{ prefix | lower}}-fc-wan-{{ vic_type }}-B"


# Additional vNICs for iSCSI boot (only configured when configure_iscsi== true)
#{{prefix}}-iSCSI-A: For iSCSI A traffic mapped to FI-A
#{{prefix}}-iSCSI-B: For iSCSI B traffic mapped to FI-B

name_of_iscsi_a_vnic                         : "{{ prefix | lower}}-iSCSI-{{ vic_type }}-A"
name_of_iscsi_b_vnic                         : "{{ prefix | lower}}-iSCSI-{{ vic_type }}-B"

#----------------------------------------------------------------------------------------------------------------------------
#------------------------- FC Configurations ------------------------------
#
#
# Boot Order Policy for Fibre Channel Boot
fc_boot_order_policy_name                    : "{{ prefix }}-FC-{{ vic_type }}-VIC-Boot-Order-Policy"
fc_boot_order_policy_description             : "Fibre Channel Boot Order Policy"


# Fibre Channel Network Policy: SAN-A
name_of_fc_network_policy_san_a         : "{{ prefix}}-FC-Network-SAN-A"
description_of_fc_network_policy_san_a  : "SAN-A Fibre Channel Network Policy using VSAN {{ vsan_A_id }}"
#fcoe_vlan_id_SAN_A                      : "{{ vsan_list[0].fcoe_vlan }}"
#vsan_id_SAN_A                           : "{{ vsan_list[0].vsan_id }}"
#fcoe_vlan_id_SAN_A                      : 101
#vsan_id_SAN_A                           : 101

# Fibre Channel Network Policy: SAN-B
name_of_fc_network_policy_san_b         : "{{ prefix}}-FC-Network-SAN-B"
description_of_fc_network_policy_san_b  : "SAN-B Fibre Channel Network Policy using VSAN {{ vsan_B_id }}"
#fcoe_vlan_id_SAN_B                      : "{{ vsan_list[1].fcoe_vlan }}"
#vsan_id_SAN_B                           : "{{ vsan_list[1].vsan_id }}"
#fcoe_vlan_id_SAN_B                      : 102
#vsan_id_SAN_B                           : 102

# Provide logical names to identify during boot
san_boot_device_1a_name      : "fcp-lif-01a"
san_boot_device_2a_name      : "fcp-lif-02a"
san_boot_device_1b_name      : "fcp-lif-01b"
san_boot_device_2b_name      : "fcp-lif-02b"

# Provide Target WWPN Names
san_boot_target_1a_wwpn      : "{{ fcp_lif_01a }}"
san_boot_target_2a_wwpn      : "{{ fcp_lif_02a }}"
san_boot_target_1b_wwpn      : "{{ fcp_lif_01b }}"
san_boot_target_2b_wwpn      : "{{ fcp_lif_02b }}"

# Fibre Channel QoS Policy
name_of_fc_qos_policy         : "{{ prefix }}-FC-QoS-Policy"
description_of_fc_qos_policy  : "Fibre Channel QoS policy with default values"

# Fibre Channel Adapter Policy
name_of_fc_adapter_policy           : "{{ prefix }}-FC-Adapter-Policy"
description_of_fc_adapter_policy    : "Fibre Channel Adapter policy with VMware platform defined default values"

# FC-NVMe Initiator Adapter Policy
name_of_fc_nvme_initiator_adapter_policy           : "{{ prefix }}-FC-NVMe-Initiator-Adapter-Policy"
description_of_fc_nvme_initiator_adapter_policy    : "FC-NVMe Initiator Adapter policy with platform defined default values"

# SAN connectivity Policy
name_of_san_connectivity_policy         : "{{ prefix }}-{{ vic_type }}-VIC-SANCon"
description_of_san_connectivity_policy  : "SAN Connectivity Policy to add FC HBAs - {{ vic_type }} VIC"
name_of_vhba_a                          : "FCP-{{ vic_type }}-Fabric-A"
name_of_vhba_b                          : "FCP-{{ vic_type }}-Fabric-B"

# Optional FC-NVMe Adapter Configuration
name_of_nvme_vhba_a                     : "FC-NVMe-{{ vic_type }}-Fabric-A"
name_of_nvme_vhba_b                     : "FC-NVMe-{{ vic_type }}-Fabric-B"

#----------------------------------------------------------------------------------------------------------------------------
#------------------------- local disk boot Configurations ------------------------------
# Boot Order Policy for local disk Boot
boot_order_policy_name: "{{ prefix }}-{{ vic_type }}-VIC-Boot-Order-Policy"
boot_order_policy_description : "Local Disk Boot Order Policy"


#----------------------------------------------------------------------------------------------------------------------------
#------------------------- iSCSI Configurations ------------------------------
# Boot Order Policy for iSCSI Boot
iscsi_boot_order_policy_name: "AA04-{{ server_generation }}-iSCSI-Boot"
  #iscsi_boot_order_policy_name: "{{ prefix }}-iSCSI-{{ vic_type }}-VIC-Boot-Order-Policy"
iscsi_boot_order_policy_description : "iSCSI Boot Order Policy"


# iSCSI Adapter Policy
name_of_iscsi_adapter_policy: "{{ prefix }}-iSCSI-Adapter-Policy"
description_of_iscsi_adapter_policy: "iSCSI Adapter policy"
tcp_connection_time_out   : 15
dhcp_timeout              : 60
lun_busy_retry_count      : 15

# ISCSI Target Infomration 
iscsi_StorageSystem_target_iqn: "{{ storage_iscsi_IQN }}"
iscsi_StorageSystem_port: 3260
iscsi_StorageSystem_lunID: 0

# iSCSI-A Primary Target Policy
name_of_iscsi_a_primary_target_policy           : "{{ prefix }}-iSCSI-A-Primary-Target"
description_of_iscsi_a_primary_target_policy    : "iSCSI-A Primary Target Policy"
target_ip_of_iscsi_a_primary                    : "{{ iscsi_lif_01a }}"

# iSCSI-A Secondary Target Policy
name_of_iscsi_a_secondary_target_policy         : "{{ prefix }}-iSCSI-A-Secondary-Target"
description_of_iscsi_a_secondary_target_policy  : "iSCSI-A Secondary Target Policy"
target_ip_of_iscsi_a_secondary                  : "{{ iscsi_lif_02a }}"

# iSCSI-B Primary Target Policy
name_of_iscsi_b_primary_target_policy           : "{{ prefix }}-iSCSI-B-Primary-Target"
description_of_iscsi_b_primary_target_policy    : "iSCSI-B Primary Target Policy"
target_ip_of_iscsi_b_primary                    : "{{ iscsi_lif_01b }}"

# iSCSI-B Secondary Target Policy
name_of_iscsi_b_secondary_target_policy         : "{{ prefix }}-iSCSI-B-Secondary-Target"
description_of_iscsi_b_secondary_target_policy  : "iSCSI-B Secondary Target Policy"
target_ip_of_iscsi_b_secondary                  : "{{ iscsi_lif_02b }}"

# iSCSI Boot Policies
name_of_iscsi_a_boot_policy                     : "{{ prefix }}-iSCSI-A-Boot-Policy"
description_of_iscsi_a_boot_policy              : "iSCSI-A Boot Policy"
name_of_iscsi_b_boot_policy                     : "{{ prefix }}-iSCSI-B-Boot-Policy"
description_of_iscsi_b_boot_policy              : "iSCSI-B Boot Policy"



#----------------------------------------------------------------------------------------------------------------------------
#------------------------- Server Profile ----------------------------------

name_of_iscsi_SPT       : "{{ prefix }}-{{ server_cpu_type }}-{{ vic_type }}-VIC-iSCSI-Boot-Template"
name_of_fc_SPT       : "{{ prefix }}-{{ server_cpu_type }}-{{ vic_type }}-VIC-FC-Boot-Template"


# "server_name_to_profile_mapping": {
#   "AA04-6454-1-1":  "{{ prefix }}-Profile-1",
#   "AA04-6454-1-2":  "{{ prefix }}-Profile-2"
#  }

#############################################################################################################################
# 
# RANCHER and RKE2/K3s variables
#
#############################################################################################################################
#
