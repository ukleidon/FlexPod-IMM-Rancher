i---
######################################################################################################################
# 
# Key Variables used to buils many other variables instead of defining them manually.
#
######################################################################################################################
#
# !!!! Intersight API key and private key must be created before running the playbool.  !!!!
# !!!! This is only required if the Organization is diffent to the one used in ucs.yml  !!!!
# #1 Please logon to Intersight and create the Organization for this tenant if required.
# #2 Create a Role with administrator privileges in the Organization.
# #3 Assign the new role to your User and switch to the new role
# #4 Create a new API key with the created role and enter the information here.
#
api_key_id        : 65c6476d3423434305833e66/65ef20a5756461310534e30c/6626720d7564613105dae462
api_private_key   : Keys/XXXkey.txt

#
# TENANT NAME is used i.e. as name for the VRF, as prefix in Intersight and so on.
# It is recommended to use only letters and numbers and no special characters.
#
# !!!! The tenant_name must match the Organization name in Intersight !!!!
# !!!! The tenant_name must match the Organization name in Intersight !!!!
tenant_name       : 'RTP4-AB02'

# Tenand ID (tid) is used as digit in various pools like UUID, MAC, WWN,...
# The tid must be a number between 00 and 99
# like for mac_pool_a - 00:25:B5:A0:{{ tid }}:[pool]
tid               : '01'

# With UCS the default network option used vNIC failover on the VIC card. There is no need to configure
# a bond device on the OS level.
# If you like or if the solution requires the use of bonding on the OS layer, set os_bond to true.
# This will disable vNIC failover and creates two vNICs for the same VLANs, one on fabric A and one
# on fabric B
# Default is false
os_bond           : 'true'

# To configure local disk boot set configure_local to true otherwise to false
# Default is false
configure_local   : 'false'

# The OS Type variable is used to defime BIOS policies, adapter policies, and LUNs.
# IT can be linux (for all kinds of linux and K8s), windows, and vmware.
# Default is vmware
os_type           : 'vmware'

# lan_state should be set to 'present' to configure the objects
# In future, this parameter will be used for deleting the configuration
lan_state         : 'present'

# In case a Proxy is required to acces the intetnet, specify the string for them.
# plese keep everything behind the - in '' 
proxys:
  - 'http_proxy   : "http://172.16.4.12:81"'
  - 'https_proxy  : "http://172.16.4.12:81"'

#
# Local User and password are used in Intersight and for the SVM admin
t_name_of_local_user                  : "esxadmin"
t_password_for_local_user             : "Respect2024!"

# Vmedia file location used for UCS to define vmedia policy for first boot
# CCD is for the boot ISO image.
# HDD is for an disk image to store autoinstall file or additional packages
vmedia_file_location_cdd          : "http://172.16.4.105/ESX-801.iso"
vmedia_name_cdd                   : 'SLES15SP5'
vmedia_file_location_hdd          : "http://172.16.4.105/autoinstall.img"
vmedia_name_hdd                   : 'AUTOINSTALL'
#
#
#---------------------------------------------------------------------------------------------------------------------
#--------------------- NetApp Storage variables ------------------------------
#
# The following variables must change after ontap_config_part_1 run
# and ucs_config run.
#
# Storage iSCSI Target IQN from the NetApp SVM created for this tenant.
# The Information is only used if configure_iscsi is set to 'true'
#
storage_iscsi_IQN: 'iqn.1992-08.com.netapp:sn.cd343aa9018111efaa2ad039ea561760:vs.6'
#
# Storage FCP WWPNs (capture from storage system)
# The Information is only used if configure_fc is set to 'true'
#
fcp_lif_01a: '20:26:d0:39:ea:29:ce:d4'
fcp_lif_02a: '20:23:d0:39:ea:29:ce:d4'
fcp_lif_01b: '20:25:d0:39:ea:29:ce:d4'
fcp_lif_02b: '20:24:d0:39:ea:29:ce:d4'
#
# Storage FC-NVMe WWPNs (capture from storage system)
# The Information is only used if configure_fc is set to 'true'
#
fc_nvme_lif_01a: '20:06:00:a0:98:e2:17:ca'
fc_nvme_lif_02a: '20:08:00:a0:98:e2:17:ca'
fc_nvme_lif_01b: '20:07:00:a0:98:e2:17:ca'
fc_nvme_lif_02b: '20:09:00:a0:98:e2:17:ca'
#
#---------------------------------------------------------------------------------------------------------------------
#------------------------- Vmware ESXi variables ----------------------------------
#
# ESXi Host Names - Separated by either FC or iSCSI. If not using the protocol, comment out the list entries.
# These host names and identifiers will be used for NetApp LUN and igroup names and Cisco MDS Device Alias names 
# and zone entries.
# They can also be assigned as UCS Profile names.

fc_esxi_hosts:
  - hostname: "aa02-esxi-1"
    fcp_a_wwpn: "20:00:00:25:b5:a2:0a:00"
    fcp_b_wwpn: "20:00:00:25:b5:a2:0b:00"
    fc_nvme_a_wwpn: "20:00:00:25:b5:a2:0a:01"
    fc_nvme_b_wwpn: "20:00:00:25:b5:a2:0b:01"
    nvme_nqn: "nqn.2014-08.com.cisco.flexpodb4:nvme:aa02-esxi-1"
  - hostname: "aa02-esxi-3"
    fcp_a_wwpn: "20:00:00:25:b5:a2:0a:02"
    fcp_b_wwpn: "20:00:00:25:b5:a2:0b:02"
    fc_nvme_a_wwpn: "20:00:00:25:b5:a2:0a:03"
    fc_nvme_b_wwpn: "20:00:00:25:b5:a2:0b:03"
    nvme_nqn: "nqn.2014-08.com.cisco.flexpodb4:nvme:aa02-esxi-3"
  - hostname: "aa02-esxi-5"
    fcp_a_wwpn: "20:00:00:25:b5:a2:0a:05"
    fcp_b_wwpn: "20:00:00:25:b5:a2:0b:04"
    fc_nvme_a_wwpn: "20:00:00:25:b5:a2:0a:04"
    fc_nvme_b_wwpn: "20:00:00:25:b5:a2:0b:05"
    nvme_nqn: "nqn.2014-08.com.cisco.flexpodb4:nvme:aa02-esxi-5"

iscsi_esxi_hosts:
  - hostname: "aa02-esxi-2"
    iscsi_iqn: "iqn.2010-11.com.flexpod:aa02-ucshost:2"
    nvme_nqn: "nqn.2014-08.com.cisco.flexpodb4:nvme:aa02-esxi-2"
  - hostname: "aa02-esxi-4"
    iscsi_iqn: "iqn.2010-11.com.flexpod:aa02-ucshost:1"
    nvme_nqn: "nqn.2014-08.com.cisco.flexpodb4:nvme:aa02-esxi-4"
  - hostname: "aa02-esxi-6"
    iscsi_iqn: "iqn.2010-11.com.flexpod:aa02-ucshost:3"
    nvme_nqn: "nqn.2014-08.com.cisco.flexpodb4:nvme:aa02-esxi-6"

# VMware Configurations
# provide the user names and passwords to connect to the ESXi servers
# and VMware vCenter
#
esxi_username: root
esxi_password: "{{ ansible_ssh_pass }}"
#
vcenter_dc: FlexPod-DC
vcenter_cluster: FlexPod-Management
#
vcenter_hostname: "10.102.1.100"
# e.g. vcenter_hostname: 10.50.160.100
vcenter_username: "administrator@vsphere.local"
# e.g. vcenter_username: administrator@vsphere.local
vcenter_password: "Respect2024!"
# e.g. vcenter_password: "cisco!23"
#
# DVS Parameters; use version 7.0.3 for VMware 7.0U3 or above
#
dvs_name: "vDS0"
iscsi_nvme_tcp_dvs_name: "iSCSI-NVMe-TCP-vDS" # Change to "iSCSI-vDS" if not using NVMe-TCP
dv_version: "7.0.3"
dv_uplink: "2"
dv_protocol: "lldp"
dv_operation: "both"
#
######################################################################################################################
#
# NetApp Storage Virtual Machine informantion for this Tenant
#
######################################################################################################################
#
# Host Names - Separated by either FC or iSCSI. If not using the protocol, comment out the list entries.
# These host names and identifiers will be used for NetApp LUN and igroup names and Cisco MDS Device Alias names 
# and zone entries.
# The hostnames can also be assigned as UCS Profile names.

t_iscsi_hosts:
  - hostname: "{{ tenant_name }}-01"
    iscsi_iqn: "iqn.2010-11.com.flexpod:{{ tenant_name }}-ucshost:1"
    nvme_nqn: "nqn.2014-08.com.cisco.flexpodb4:nvme:{{ tenant_name }}-01"
#
# Storage NVMe-TCP Target Interfaces
#
nvme_lif_01a: "{{ t_nvme_tcpA_network_prefix }}.11"
nvme_lif_01b: "{{ t_nvme_tcpB_network_prefix }}.11"
nvme_lif_02a: "{{ t_nvme_tcpA_network_prefix }}.12"
nvme_lif_02b: "{{ t_nvme_tcpB_network_prefix }}.12"
#
# Storage iSCSI Target Interfaces
#
iscsi_lif_01a: "{{ t_iscsiA_network_prefix }}.11"
iscsi_lif_01b: "{{ t_iscsiB_network_prefix }}.11"
iscsi_lif_02a: "{{ t_iscsiA_network_prefix }}.12"
iscsi_lif_02b: "{{ t_iscsiB_network_prefix }}.12"
#
# Storage NFS Target Interfaces
#
nfs_lif_01: "{{ t_nfs_network_prefix }}.11"
nfs_lif_02: "{{ t_nfs_network_prefix }}.12"
#
#---------------------------------------------------------------------------------------------------------------------
#
# NetApp Storage Virtual Machine informantion
#
cluster_name       : 'c250-01'                   # NetApp cluster name, also used as ansible_host in inventory !!!
aggr_prefix        : 'c250_01'                   # Should be same as cluster name
svm_specs:
  svm_name: "{{ tenant_name }}_svm"
  svm_root_vol: "{{ tenant_name }}_svm_root"
  allowed_protocols:
    #provide the values in lower case only, supported options for this solution are nfs, fcp, iscsi, nvme
    #For FC-NVMe config, use fcp and nvme
    #For NVMe/TCP config, use nvme and iscsi
    - nfs
#    - fcp  
#    - nvme
    - iscsi
  data_protocol: nfs
  client_match: "{{ t_nfs_network_prefix }}.0/{{ t_nfs_network_mask }}"
  data_volumes_file: # If name includes swap - volume efficientcy will be disabled
    - {name: "{{tenant_name}}_datastore", size: 1024, residing_aggr: "{{ aggr_prefix }}n2_aggr1"}
    - {name: "{{tenant_name}}_swap", size: 1024, residing_aggr: "{{ aggr_prefix }}n1_aggr1"}
    - {name: "{{tenant_name}}_vCLS", size: 100, residing_aggr: "{{ aggr_prefix }}n2_aggr1"}
  data_volumes_block:
    - {name: "{{tenant_name}}_boot", size: 1024, residing_aggr: "{{ aggr_prefix }}n1_aggr1"}
      #    - {name: "{{tenant_name}}_nvme", size: 1024, residing_aggr: c250_02n2_aggr1}
  nvme_subsystem: "{tenant_name}}_nvme_hosts"
  nvme_namespaces:  #Mention the namespaces that you want to create and map to the subsystem
    - {name: "{{tenant_name}}_nvme_01", size: 500, residing_vol: "{{tenant_name}}_nvme"}
  #host names will be used for NetApp Boot LUNs and igroup names. These vars are placed under group_vars/all.yml file
  boot_luns_iscsi: {size: 128, residing_vol: "{{tenant_name}}_boot"}
  svm_mgmt_lif: {home_node: "{{ cluster }}n2", address: "{{ t_ib_network_prefix }}.101", netmask: "{{ t_ib_network_mask }}"}, gateway: "{{ t_ib_network_prefix }}.1", lif_name: "{{ tenant_name }}-svm-mgmt"}
  vsadmin_password: "{{ t_password_for_local_user }}"
  os_type: "{{ os_type }}"
  dns_server_svm:
    - "{{ dns_servers[0].ip_address }}"
    - "{{ dns_servers[1].ip_address }}"
  dns_domain_svm: "{{ dns_domain_name }}"
  svm_login_banner: This SVM is reserved for authorized users only!  #SVM Login banner Text message
  audit_log_volume_specs: {name: "{{tenant_name}}_audit_log", size: 50, residing_aggr: "{{ aggr_prefix }}n1_aggr1"}
            #Audit log destination volume where consolidated audit logs will be stored

svm_node_specs:
  - node_name: "{{ cluster_name }}n1"
    nfs_lifs:  {name: "{{ tenant_name }}-nfs-lif-01a", address: "{{ nfs_lif_01 }}", netmask: "{{ t_nfs_network_netmask }}"}
    fcp_lifs:    #Fill out this value only if fcp will be mentioned under allowed_protocols in svm_specs
      - {name: "{{ tenant_name }}-fcp-lif-01a", home_port: 5a, fabric: A}  #Do not change the fabric ID
      - {name: "{{ tenant_name }}-fcp-lif-01b", home_port: 5b, fabric: B}  #Do not change the fabric ID
    fc-nvme_lifs:    #Fill out this value only if fcp and nvme will be mentioned under allowed_protocols in svm_specs
      - {name: "{{ tenant_name }}-fc-nvme-lif-01a", home_port: 2a, fabric: A}  #Do not change the fabric ID
      - {name: "{{ tenant_name }}-fc-nvme-lif-01b", home_port: 2b, fabric: B}  #Do not change the fabric ID
    iscsi_lifs:  #Fill out this value only if iscsi will be mentioned under allowed_protocols in svm_specs. 
                                                                                        #Do not change the fabric ID
      - {name: "{{ tenant_name }}-iscsi-lif-01a", address: "{{ iscsi_lif_01a }}", netmask: "{{ t_iscsiA_network_netmask }}", fabric: A}
      - {name: "{{ tenant_name }}-iscsi-lif-01b", address: "{{ iscsi_lif_01b }}", netmask: "{{ t_iscsiB_network_netmask }}", fabric: B}
    nvme_tcp_lifs:  #Fill out this value only if iscsi and nvme will be mentioned under allowed_protocols 
                                                                                          #Do not change the fabric ID
      - {name: "{{ tenant_name }}-nvme-tcp-lif-01a", address: "{{ nvme_lif_01a }}", netmask: "{{ t_nvme_tcpA_network_netmask }}", fabric: A}
      - {name: "{{ tenant_name }}-nvme-tcp-lif-01b", address: "{{ nvme_lif_01b }}", netmask: "{{ t_nvme_tcpB_network_netmask }}", fabric: B}
  - node_name: "{{ cluster_name }}n2"
    nfs_lifs:  {name: "{{ tenant_name }}-nfs-lif-02b", address: "{{ nfs_lif_02 }}", netmask: "{{ t_nfs_network_netmask }}"}
    fcp_lifs:    #Fill out this value only if fcp will be mentioned under allowed_protocols in svm_specs
      - {name: "{{ tenant_name }}-fcp-lif-02a", home_port: 5a, fabric: A}  #Do not change the fabric ID
      - {name: "{{ tenant_name }}-fcp-lif-02b", home_port: 5b, fabric: B}  #Do not change the fabric ID
    fc-nvme_lifs:    #Fill out this value only if fcp and nvme will be mentioned under allowed_protocols 
      - {name: "{{ tenant_name }}-fc-nvme-lif-02a", home_port: 2a, fabric: A}  #Do not change the fabric ID
      - {name: "{{ tenant_name }}-fc-nvme-lif-02b", home_port: 2b, fabric: B}  #Do not change the fabric ID
    iscsi_lifs:  #Fill out this value only if iscsi will be mentioned under allowed_protocols in svm_specs. 
                                                                                          #Do not change the fabric ID
      - {name: "{{ tenant_name }}-iscsi-lif-02a", address: "{{ iscsi_lif_02a }}", netmask: "{{ t_iscsiA_network_netmask }}", fabric: A}
      - {name: "{{ tenant_name }}-iscsi-lif-02b", address: "{{ iscsi_lif_02b }}", netmask: "{{ t_iscsiB_network_netmask }}", fabric: B}
    nvme_tcp_lifs:  #Fill out this value only if iscsi and nvme will be mentioned under allowed_protocols 
                                                                                          #Do not change the fabric ID
      - {name: "{{ tenant_name }}-nvme-tcp-lif-02a", address: "{{ nvme_lif_02a }}", netmask: "{{ t_nvme_tcpA_network_netmask }}", fabric: A}
      - {name: "{{ tenant_name }}-nvme-tcp-lif-02b", address: "{{ nvme_lif_02b }}", netmask: "{{ t_nvme_tcpB_network_netmask }}", fabric: B}


######################################################################################################################
# This is the list of all the VLANs that will be defined on Nexus, Storage, UCS, and VMware
# VLAN Names are adjustable and can be modified in here
# Comment out any VLANs that are not used here and below (iSCSI and NVMe-TCP)
#
## t_XXXX_name          : Defines the Name of the network
# t_XXXX_id            : Defines the VLAN ID of the network
# t_XXXX_network_prefix: Defines the first three digits of the network ip address.
#                        Host IPs, Storage IPs, and Routing IPs are automaticaliy added to the prefix
#                        Router IP is set as default to .1
#                        SVI IP on n9kA is .2 and on n9kB is .2
#                        NetApp SVM IPs are set to .101 o node 1 and .102 on node 2.
# t_XXXX_network_mask  : Defines the netmask of the network
#
t_ib_vlan_name:            "{{ tenant_name }}-Management_LAN"   # IN BAND Management VLAN
t_ib_vlan_id:              300                                  # VLAN ID for IN-BAND Management
t_ib_network_prefix:       '172.18.0.'                          # first three digits of the network adress
t_ib_network_mask:         24                                   # Netmask
_ib_network_netmask:       255.255.252.0                        # Netmask for this network
#
t_iscsiA_vlan_name:        "{{ tenant_name }}-iSCSI-A"          # iSCSI-A VLAN (if needed)
t_iscsiA_vlan_id:          "{{ t_ib_vlan_id + 1 }}"             # Management VLAN +1
t_iscsiA_network_prefix:   '172.18.1'                           # first three digits of the network adress
t_iscsiA_network_mask:     24                                   # Netmask
t_iscsiA_network_netmask:  255.255.255.0                        # Netmask for this network
#
t_iscsiB_vlan_name:        "{{ tenant_name }}-iSCSI-B"          # iSCSI-B VLAN (if needed)
t_iscsiB_vlan_id:          "{{ t_ib_vlan_id + 2 }}"             # Management VLAN +2
t_iscsiB_network_prefix:   '172.18.2'                           # first three digits of the network adress
t_iscsiB_network_mask:     24                                   # Netmask
t_iscsiB_network_netmask:  255.255.255.0                        # Netmask for this network
#
t_nfs_vlan_name:           "{{ tenant_name }}-NFS"              # NFS Traffic between ESXi and Storage
t_nfs_vlan_id:             "{{ t_ib_vlan_id + 3 }}"             # Management VLAN +3
t_nfs_network_prefix:      '172.18.3'                           # first three digits of the network adress
t_nfs_network_mask:        24                                   # Netmask
t_nfs_network_netmask:     255.255.255.0                        # Netmask for this network
 #
t_access_vlan_name:        "{{ tenant_name }}-Access-Traffic"   # VLAN to carry Access traffic to RKE2 payload
t_access_vlan_id:          "{{ t_ib_vlan_id + 4 }}"             # Management VLAN +4
t_access_network_prefix:   '172.18.4'                           # first three digits of the network adress
t_access_network_mask:     24                                   # Netmask
t_access_network_netmask:  255.255.255.0                        # Netmask for this network
#
t_vm_vlan_name:            "{{ tenant_name }}-VM-Traffic"       # VLAN to carry VM traffic on VDS
t_vm_vlan_id:              "{{ t_ib_vlan_id + 5 }}"             # Management VLAN +5
t_vm_network_prefix:       '172.18.5'                           # first three digits of the network adress
t_vm_network_mask:         24                                   # Netmask
t_vm_network_netmask:      255.255.255.0                        # Netmask for this network
 #
t_vmotion_vlan_name:       "{{ tenant_name }}-vMotion"          # vMotion VLAN
t_vmotion_vlan_id:         "{{ t_ib_vlan_id + 6 }}"             # Management VLAN +6
t_vmotion_network_prefix:  '172.18.6'                           # first three digits of the network adress
t_vmotion_network_mask:    24                                   # Netmask
t_vmotion_network_netmask: 255.255.255.0                        # Netmask for this network
#
t_nvme_tcpA_vlan_name:       "{{ tenant_name }}-Infra-NVMe-TCP-A" # NVMe-TCP-A VLAN (if needed)
t_nvme_tcpA_vlan_id:         "{{ t_ib_vlan_id + 7 }}"             # Management VLAN +7
t_nvme_tcpA_network_prefix:  '172.18.7'                           # first three digits of the network adress
t_nvme_tcpB_network_mask:    24                                   # Netmask
t_nvme_tcpA_network_netmask: 255.255.255.0                        # Netmask for this network
#
t_nvme_tcpB_vlan_name:       "{{ tenant_name }}-Infra-NVMe-TCP-B" # NVMe-TCP-B VLAN (if needed)
t_nvme_tcpB_vlan_id:         "{{ t_ib_vlan_id + 8 }}"             # Management VLAN +8
t_nvme_tcpB_network_prefix:  '172.18.8'                           # first three digits of the network adress
t_nvme_tcpB_network_mask:    24                                   # Netmask
t_nvme_tcpB_network_netmask: 255.255.255.0                        # Netmask for this network


#
#---------------------------------------------------------------------------------------------------------------------
#
# VLAN Lists - Comment out or remove any VLANs not being used.
#
# The ib_mgmt_vlan_list contains one entry, the IB-MGMT VLAN.
#
t_ib_mgmt_vlan_list:
  - name: "{{ t_ib_vlan_name }}"
    id: "{{ t_ib_vlan_id }}"
    native: 'no'
    state: "{{lan_state}}"
#
# The storage_vlan_list contains VLANs that are configured on the storage controllers.
# These VLANs are also configured in the UCS and in the Nexus switches. This list has
# two extra fields, storage_protocol, and fabric. Do not remove these extra fields.
#
t_storage_vlan_list:
  - name: "{{ t_nfs_vlan_name }}"
    id: "{{ t_nfs_vlan_id }}"
    native: 'no'
    storage_protocol: NFS
    state: "{{lan_state}}"
    # ISCSI A and B VLANs should be deleted or commended out for Fiber-Channel-Only deployments
  - name: "{{ t_iscsiA_vlan_name }}"
    id: "{{ t_iscsiA_vlan_id }}"
    native: 'no'
    storage_protocol: iSCSI
    fabric: A
    state: "{{lan_state}}"
  - name: "{{ t_iscsiB_vlan_name }}"
    id: "{{ t_iscsiB_vlan_id }}"
    native: 'no'
    storage_protocol: iSCSI
    fabric: B
    state: "{{lan_state}}"
  - name: "{{ t_nvme_tcpA_vlan_name }}"
    id: "{{ t_nvme_tcpA_vlan_id }}"
    native: 'no'
    storage_protocol: NVMe
    fabric: A
    state: "{{lan_state}}"
  - name: "{{ t_nvme_tcpB_vlan_name }}"
    id: "{{ t_nvme_tcpB_vlan_id }}"
    native: 'no'
    storage_protocol: NVMe
    fabric: B
    state: "{{lan_state}}"

# The remaining_vlan_list contains all vlans that are not configured on storage,
# but are configured in Nexus and UCS.
#
t_remaining_vlan_list:
  - name: "{{ t_vm_vlan_name }}"
    id: "{{ t_vm_vlan_id }}"
    native: 'no'
    state: "{{lan_state}}"
  - name: "{{ t_vmotion_vlan_name }}"
    id: "{{ t_vmotion_vlan_id }}"
    native: 'no'
    state: "{{lan_state}}"

# VLANs definitions below will be used to setup trunk ports on Nexus Switches
# all_vlans_list: for vpc_peer_link and UCS FI trunk ports
# These VLANs must be same or a subset of the vlan_list abovea
# Comment out or remove any VLANs that are not used (iSCSI and NVMe-TCP)
# In future, these values will be auto-generated
#
t_all_vlans_list: "{{ oob_vlan_id }},{{ t_ib_vlan_id }},{{ t_vm_vlan_id }},{{ t_nfs_vlan_id }},{{ t_vmotion_vlan_id }},{{ t_iscsiA_vlan_id }},{{ t_iscsiB_vlan_id }},{{ t_nvme_tcpA_vlan_id }},{{ t_nvme_tcpB_vlan_id }},{{ t_access_vlan_id }}"
# storage_vlans_list: for storage uplink trunk ports
t_storage_vlans_list: "{{ t_ib_vlan_id }},{{ t_nfs_vlan_id }},{{ t_iscsiA_vlan_id }},{{ t_iscsiB_vlan_id }},{{ t_nvme_tcpA_vlan_id }},{{ t_nvme_tcpB_vlan_id }}"
# mgmt_vlans_list: for uplink/management switch trunk port
t_mgmt_vlans_list: "{{ oob_vlan_id }},{{ t_ib_vlan_id }},{{ t_vm_vlan_id }}"
#
#
# The SVI list defines IP and VRF information to enable routing function on the Nexus Switches.
# The baseIP is defined in the nexus host configuration file in the hosts_var directory.
# 
t_svi_list:
  - name:  "{{ t_ib_vlan_id }}"
    vrf: "vmware"
    address: "{{ t_ib_network_prefix}}.{{ baseIP }}/{{ t_ib_network_mask }}"
    hsrp: "{{ t_ib_network_prefix}}.1"
  - name:  "{{ t_iscsiA_vlan_id }}"
    vrf: "vmware"
    address: "{{ t_iscsiA_network_prefix }}.{{ baseIP }}/{{ t_iscsiA_network_mask }}"
    hsrp: "{{ t_iscsiA_network_prefix }}.1"
  - name:  "{{ t_iscsiB_vlan_id }}"
    vrf: "vmware"
    address: "{{ t_iscsiB_network_prefix }}.{{ baseIP }}/{{ t_iscsiB_network_mask }}"
    hsrp: "{{ t_iscsiB_network_prefix }}.1"
  - name:  "{{ t_nfs_vlan_id }}"
    vrf: "vmware"
    address: "{{ t_nfs_network_prefix }}.{{ baseIP }}/{{ t_nfs_network_mask }}"
    hsrp: "{{ t_nfs_network_prefix }}.1"
  - name:  "{{ t_access_vlan_id }}"
    vrf: "vmware"
    address: "{{ t_access_network_prefix }}.{{ baseIP }}/{{ t_access_network_mask }}"
    hsrp: "{{ t_access_network_prefix }}.1"
  - name:  "{{ t_vm_vlan_id }}"
    vrf: "vmware"
    address: "{{ t_vm_network_prefix }}.{{ baseIP }}/{{ t_vm_network_mask }}"
    hsrp: "{{ t_vm_network_prefix }}.1"
  - name:  "{{ t_vmotion_vlan_id }}"
    vrf: "vmware"
    address: "{{ t_vmotion_network_prefix }}.{{ baseIP }}/{{ t_vmotion_network_mask }}"
    hsrp: "{{ t_vmotion_network_prefix }}.1"

# VLAN/Portgroup List with Pinning info for VMware Distributed Switch vDSO for vMotion and Tenant Data/Storage
vds0_vlan_list:
  - name: "{{ t_vm_vlan_name }}"
    # portgroup name will be configured in VMware
    portgroup_name: "{{ t_vm_vlan_name }}"
    id: "{{ t_vm_vlan_id }}"
    active_uplinks: 
      - "Uplink 1"
      - "Uplink 2"
    standby_uplinks: []
  - name: "{{ t_vmotion_vlan_name }}"
    # portgroup name will be configured in VMware
    portgroup_name: "{{ t_vmotion_vlan_name }}"
    id: "{{ t_vmotion_vlan_id }}"
    active_uplinks: "Uplink 2"
    standby_uplinks: "Uplink 1"
#
# VLAN/Portgroup List with Pinning info for VMware Distributed Switch iSCSI-NVMe-TCP-vDS for iSCSI and NVMe Storage
iscsi_nvme_tcp_vds_vlan_list:
  - name: "{{ t_iscsiA_vlan_name }}"
    # portgroup name will be configured in VMware
    portgroup_name: "{{ t_iscsiA_vlan_name }}"
    id: '0'  # iSCSI VLAN A is set as native for this vNIC
    active_uplinks:
      - "Uplink 1"
    standby_uplinks: []
  - name: "{{ t_iscsiB_vlan_name }}"
    # portgroup name will be configured in VMware
    portgroup_name: "{{ t_iscsiB_vlan_name }}"
    id: '0'  # iSCSI VLAN B is set as native for this vNIC
    active_uplinks:
      - "Uplink 2"
    standby_uplinks: []
  - name: "{{ t_nvme_tcpA_vlan_name }}"
    # portgroup name will be configured in VMware
    portgroup_name: "{{ t_nvme_tcpA_vlan_name }}"
    id: "{{ t_nvme_tcpA_vlan_id }}"
    active_uplinks:
      - "Uplink 1"
    standby_uplinks: []
  - name: "{{ t_nvme_tcpB_vlan_name }}"
    # portgroup name will be configured in VMware
    portgroup_name: "{{ t_nvme_tcpB_vlan_name }}"
    id: "{{ t_nvme_tcpB_vlan_id }}"
    active_uplinks:
      - "Uplink 2"
    standby_uplinks: []

nfs_portgroup: "VMKernel-Infra-NFS"
esxi_nfs_vlan: "{{ t_nfs_vlan_id }}"
#
oob_mgmt_portgroup: "OOB-MGMT Network"
oob_mgmt_vlan: "{{ oob_vlan_id }}"
#
#VSAN Parameters
#
t_vsan_A_name: 'FlexPod-Fabric-A'
t_vsan_A_id: 101
t_vsan_A_fcoe_vlan: 101
#
t_vsan_B_name: 'FlexPod-Fabric-B'
t_vsan_B_id: 102
t_vsan_B_fcoe_vlan: 102
#
